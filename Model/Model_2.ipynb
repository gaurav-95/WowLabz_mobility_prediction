{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9eb3949",
   "metadata": {},
   "source": [
    "Source: https://www.programmersought.com/article/45455885835/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21ac8752",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Gaurav\\Downloads\\WowLabz\\WowLabz_mobility_prediction\\Dataset\"\n",
    "file = r\"\\go_track_trackspoints.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0c4cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Gaurav\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Number of samples: 18107, dimension: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " (18099, 6, 2)\n",
      "y\n",
      " (18099, 2)\n",
      "Train on 18099 samples\n",
      "Epoch 1/100\n",
      "18099/18099 [==============================] - 4s 209us/sample - loss: 0.0180 - acc: 0.9466\n",
      "Epoch 2/100\n",
      "18099/18099 [==============================] - 4s 199us/sample - loss: 0.0046 - acc: 0.9587\n",
      "Epoch 3/100\n",
      "18099/18099 [==============================] - 4s 196us/sample - loss: 0.0040 - acc: 0.9615\n",
      "Epoch 4/100\n",
      "18099/18099 [==============================] - 4s 200us/sample - loss: 0.0035 - acc: 0.9633\n",
      "Epoch 5/100\n",
      "18099/18099 [==============================] - 4s 203us/sample - loss: 0.0033 - acc: 0.9653\n",
      "Epoch 6/100\n",
      "18099/18099 [==============================] - 3s 193us/sample - loss: 0.0030 - acc: 0.9659\n",
      "Epoch 7/100\n",
      "18099/18099 [==============================] - 3s 191us/sample - loss: 0.0028 - acc: 0.9645\n",
      "Epoch 8/100\n",
      "18099/18099 [==============================] - 3s 192us/sample - loss: 0.0026 - acc: 0.9637s - loss: 0.0026 - acc: 0\n",
      "Epoch 9/100\n",
      "18099/18099 [==============================] - 3s 188us/sample - loss: 0.0024 - acc: 0.9641\n",
      "Epoch 10/100\n",
      "18099/18099 [==============================] - 3s 188us/sample - loss: 0.0022 - acc: 0.9672\n",
      "Epoch 11/100\n",
      "18099/18099 [==============================] - 3s 189us/sample - loss: 0.0019 - acc: 0.9672\n",
      "Epoch 12/100\n",
      "18099/18099 [==============================] - 3s 190us/sample - loss: 0.0017 - acc: 0.9678\n",
      "Epoch 13/100\n",
      "18099/18099 [==============================] - 4s 194us/sample - loss: 0.0015 - acc: 0.9683\n",
      "Epoch 14/100\n",
      "18099/18099 [==============================] - 4s 196us/sample - loss: 0.0014 - acc: 0.9673\n",
      "Epoch 15/100\n",
      "18099/18099 [==============================] - 3s 188us/sample - loss: 0.0012 - acc: 0.9690\n",
      "Epoch 16/100\n",
      "18099/18099 [==============================] - 3s 189us/sample - loss: 0.0010 - acc: 0.9693\n",
      "Epoch 17/100\n",
      "18099/18099 [==============================] - 3s 186us/sample - loss: 9.2000e-04 - acc: 0.9685\n",
      "Epoch 18/100\n",
      "18099/18099 [==============================] - 3s 192us/sample - loss: 8.2626e-04 - acc: 0.9702\n",
      "Epoch 19/100\n",
      "18099/18099 [==============================] - 4s 199us/sample - loss: 7.1132e-04 - acc: 0.9692\n",
      "Epoch 20/100\n",
      "18099/18099 [==============================] - 4s 196us/sample - loss: 6.8077e-04 - acc: 0.9713\n",
      "Epoch 21/100\n",
      "18099/18099 [==============================] - 3s 181us/sample - loss: 5.5992e-04 - acc: 0.9709\n",
      "Epoch 22/100\n",
      "18099/18099 [==============================] - 3s 185us/sample - loss: 5.3647e-04 - acc: 0.9707\n",
      "Epoch 23/100\n",
      "18099/18099 [==============================] - 3s 185us/sample - loss: 4.7400e-04 - acc: 0.9736\n",
      "Epoch 24/100\n",
      "18099/18099 [==============================] - 4s 197us/sample - loss: 5.1590e-04 - acc: 0.9712\n",
      "Epoch 25/100\n",
      "18099/18099 [==============================] - 4s 197us/sample - loss: 4.8211e-04 - acc: 0.9722\n",
      "Epoch 26/100\n",
      "18099/18099 [==============================] - 3s 185us/sample - loss: 4.1662e-04 - acc: 0.9720\n",
      "Epoch 27/100\n",
      "18099/18099 [==============================] - 3s 185us/sample - loss: 4.1555e-04 - acc: 0.9723\n",
      "Epoch 28/100\n",
      "18099/18099 [==============================] - 3s 186us/sample - loss: 4.1472e-04 - acc: 0.9738\n",
      "Epoch 29/100\n",
      "18099/18099 [==============================] - 3s 189us/sample - loss: 4.1216e-04 - acc: 0.9741\n",
      "Epoch 30/100\n",
      "18099/18099 [==============================] - 3s 193us/sample - loss: 4.0232e-04 - acc: 0.9726\n",
      "Epoch 31/100\n",
      "18099/18099 [==============================] - 3s 193us/sample - loss: 3.7467e-04 - acc: 0.9748\n",
      "Epoch 32/100\n",
      "18099/18099 [==============================] - 4s 198us/sample - loss: 3.8432e-04 - acc: 0.9741\n",
      "Epoch 33/100\n",
      "18099/18099 [==============================] - 4s 196us/sample - loss: 3.8594e-04 - acc: 0.9744\n",
      "Epoch 34/100\n",
      "18099/18099 [==============================] - 4s 196us/sample - loss: 3.9912e-04 - acc: 0.9740\n",
      "Epoch 35/100\n",
      "18099/18099 [==============================] - 3s 191us/sample - loss: 3.5828e-04 - acc: 0.9739\n",
      "Epoch 36/100\n",
      "18099/18099 [==============================] - 4s 195us/sample - loss: 3.4486e-04 - acc: 0.9733\n",
      "Epoch 37/100\n",
      "18099/18099 [==============================] - 4s 208us/sample - loss: 3.3796e-04 - acc: 0.9734\n",
      "Epoch 38/100\n",
      "18099/18099 [==============================] - 4s 194us/sample - loss: 3.3048e-04 - acc: 0.9764\n",
      "Epoch 39/100\n",
      "18099/18099 [==============================] - 4s 207us/sample - loss: 3.3278e-04 - acc: 0.9735\n",
      "Epoch 40/100\n",
      "18099/18099 [==============================] - 4s 200us/sample - loss: 3.5575e-04 - acc: 0.9738\n",
      "Epoch 41/100\n",
      "18099/18099 [==============================] - 4s 211us/sample - loss: 3.2993e-04 - acc: 0.9750\n",
      "Epoch 42/100\n",
      "18099/18099 [==============================] - 4s 241us/sample - loss: 3.2563e-04 - acc: 0.9739\n",
      "Epoch 43/100\n",
      "18099/18099 [==============================] - 4s 211us/sample - loss: 3.3907e-04 - acc: 0.9762\n",
      "Epoch 44/100\n",
      "18099/18099 [==============================] - 3s 191us/sample - loss: 3.1642e-04 - acc: 0.9745\n",
      "Epoch 45/100\n",
      "18099/18099 [==============================] - 3s 192us/sample - loss: 3.2524e-04 - acc: 0.9748\n",
      "Epoch 46/100\n",
      "18099/18099 [==============================] - 3s 183us/sample - loss: 3.1098e-04 - acc: 0.9754\n",
      "Epoch 47/100\n",
      "18099/18099 [==============================] - 3s 188us/sample - loss: 3.0552e-04 - acc: 0.9733\n",
      "Epoch 48/100\n",
      "18099/18099 [==============================] - 3s 186us/sample - loss: 3.0206e-04 - acc: 0.9759\n",
      "Epoch 49/100\n",
      "18099/18099 [==============================] - 3s 191us/sample - loss: 2.9888e-04 - acc: 0.9758\n",
      "Epoch 50/100\n",
      "18099/18099 [==============================] - 3s 188us/sample - loss: 3.0658e-04 - acc: 0.9765\n",
      "Epoch 51/100\n",
      "18099/18099 [==============================] - 3s 189us/sample - loss: 3.1741e-04 - acc: 0.9746\n",
      "Epoch 52/100\n",
      "18099/18099 [==============================] - 3s 186us/sample - loss: 3.2279e-04 - acc: 0.9741\n",
      "Epoch 53/100\n",
      "18099/18099 [==============================] - ETA: 0s - loss: 3.0010e-04 - acc: 0.976 - 3s 178us/sample - loss: 2.9985e-04 - acc: 0.9761\n",
      "Epoch 54/100\n",
      "18099/18099 [==============================] - 3s 186us/sample - loss: 3.1077e-04 - acc: 0.9745\n",
      "Epoch 55/100\n",
      "18099/18099 [==============================] - 3s 185us/sample - loss: 2.9330e-04 - acc: 0.9746\n",
      "Epoch 56/100\n",
      "18099/18099 [==============================] - 3s 192us/sample - loss: 2.9998e-04 - acc: 0.9743\n",
      "Epoch 57/100\n",
      "18099/18099 [==============================] - 3s 186us/sample - loss: 2.8857e-04 - acc: 0.9771\n",
      "Epoch 58/100\n",
      "18099/18099 [==============================] - 3s 191us/sample - loss: 3.0343e-04 - acc: 0.9754\n",
      "Epoch 59/100\n",
      "18099/18099 [==============================] - 4s 196us/sample - loss: 2.9749e-04 - acc: 0.9754\n",
      "Epoch 60/100\n",
      "18099/18099 [==============================] - 3s 180us/sample - loss: 3.1129e-04 - acc: 0.9766\n",
      "Epoch 61/100\n",
      "18099/18099 [==============================] - 3s 180us/sample - loss: 2.9405e-04 - acc: 0.9762\n",
      "Epoch 62/100\n",
      "18099/18099 [==============================] - 3s 191us/sample - loss: 3.0069e-04 - acc: 0.9755s - loss: 2.8235e-\n",
      "Epoch 63/100\n",
      "18099/18099 [==============================] - 3s 186us/sample - loss: 3.0628e-04 - acc: 0.9757\n",
      "Epoch 64/100\n",
      "18099/18099 [==============================] - 3s 181us/sample - loss: 2.9498e-04 - acc: 0.9746\n",
      "Epoch 65/100\n",
      "18099/18099 [==============================] - 3s 182us/sample - loss: 2.9902e-04 - acc: 0.9750\n",
      "Epoch 66/100\n",
      "18099/18099 [==============================] - 3s 185us/sample - loss: 2.9565e-04 - acc: 0.9752\n",
      "Epoch 67/100\n",
      "18099/18099 [==============================] - 3s 182us/sample - loss: 3.0534e-04 - acc: 0.9741\n",
      "Epoch 68/100\n",
      "18099/18099 [==============================] - 3s 183us/sample - loss: 3.0041e-04 - acc: 0.9769\n",
      "Epoch 69/100\n",
      "18099/18099 [==============================] - 3s 182us/sample - loss: 2.8681e-04 - acc: 0.9761\n",
      "Epoch 70/100\n",
      "18099/18099 [==============================] - 3s 185us/sample - loss: 3.0119e-04 - acc: 0.9768\n",
      "Epoch 71/100\n",
      "18099/18099 [==============================] - 3s 189us/sample - loss: 2.8853e-04 - acc: 0.9752\n",
      "Epoch 72/100\n",
      "18099/18099 [==============================] - 3s 192us/sample - loss: 2.8908e-04 - acc: 0.9759\n",
      "Epoch 73/100\n",
      "18099/18099 [==============================] - 3s 183us/sample - loss: 2.9018e-04 - acc: 0.9742\n",
      "Epoch 74/100\n",
      "18099/18099 [==============================] - 3s 183us/sample - loss: 2.8442e-04 - acc: 0.9761\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18099/18099 [==============================] - 3s 185us/sample - loss: 3.0791e-04 - acc: 0.9744\n",
      "Epoch 76/100\n",
      "18099/18099 [==============================] - 3s 188us/sample - loss: 2.8500e-04 - acc: 0.9764\n",
      "Epoch 77/100\n",
      "18099/18099 [==============================] - 4s 194us/sample - loss: 3.0604e-04 - acc: 0.9762\n",
      "Epoch 78/100\n",
      "18099/18099 [==============================] - 3s 193us/sample - loss: 2.8827e-04 - acc: 0.9756\n",
      "Epoch 79/100\n",
      "18099/18099 [==============================] - 3s 183us/sample - loss: 2.8948e-04 - acc: 0.9760\n",
      "Epoch 80/100\n",
      "18099/18099 [==============================] - 3s 187us/sample - loss: 3.0131e-04 - acc: 0.9749\n",
      "Epoch 81/100\n",
      "18099/18099 [==============================] - 4s 193us/sample - loss: 2.8594e-04 - acc: 0.9741\n",
      "Epoch 82/100\n",
      "18099/18099 [==============================] - 3s 188us/sample - loss: 2.8837e-04 - acc: 0.9758\n",
      "Epoch 83/100\n",
      "18099/18099 [==============================] - 3s 190us/sample - loss: 2.9147e-04 - acc: 0.9750\n",
      "Epoch 84/100\n",
      "18099/18099 [==============================] - 3s 186us/sample - loss: 2.9183e-04 - acc: 0.9755\n",
      "Epoch 85/100\n",
      "18099/18099 [==============================] - 3s 184us/sample - loss: 2.9231e-04 - acc: 0.9749\n",
      "Epoch 86/100\n",
      "18099/18099 [==============================] - 3s 189us/sample - loss: 2.8238e-04 - acc: 0.9744\n",
      "Epoch 87/100\n",
      "18099/18099 [==============================] - 4s 194us/sample - loss: 2.8066e-04 - acc: 0.9757\n",
      "Epoch 88/100\n",
      "18099/18099 [==============================] - 3s 191us/sample - loss: 2.8372e-04 - acc: 0.9755\n",
      "Epoch 89/100\n",
      "18099/18099 [==============================] - 3s 187us/sample - loss: 2.8341e-04 - acc: 0.9772\n",
      "Epoch 90/100\n",
      "18099/18099 [==============================] - 3s 189us/sample - loss: 2.8135e-04 - acc: 0.9757\n",
      "Epoch 91/100\n",
      "18099/18099 [==============================] - 3s 187us/sample - loss: 2.8241e-04 - acc: 0.9744\n",
      "Epoch 92/100\n",
      "18099/18099 [==============================] - 3s 192us/sample - loss: 2.8678e-04 - acc: 0.9731\n",
      "Epoch 93/100\n",
      "18099/18099 [==============================] - 4s 239us/sample - loss: 2.8109e-04 - acc: 0.9744\n",
      "Epoch 94/100\n",
      "18099/18099 [==============================] - 4s 217us/sample - loss: 2.8388e-04 - acc: 0.9752\n",
      "Epoch 95/100\n",
      "18099/18099 [==============================] - 4s 218us/sample - loss: 2.7551e-04 - acc: 0.9754\n",
      "Epoch 96/100\n",
      "12864/18099 [====================>.........] - ETA: 1s - loss: 3.0409e-04 - acc: 0.9757"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import Callback\n",
    "#import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import  keras.callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    " \n",
    "def create_dataset(data,n_predictions,n_next):\n",
    "    '''\n",
    "         Process the data\n",
    "    '''\n",
    "    dim = data.shape[1]\n",
    "    train_X, train_Y = [], []\n",
    "    for i in range(data.shape[0]-n_predictions-n_next-1):\n",
    "        a = data[i:(i+n_predictions), :]\n",
    "        train_X.append(a)\n",
    "        tempb = data[(i+n_predictions):(i+n_predictions+n_next), :]\n",
    "        b = []\n",
    "        for j in range(len(tempb)):\n",
    "            for k in range(dim):\n",
    "                b.append(tempb[j, k])\n",
    "        train_Y.append(b)\n",
    "    train_X = np.array(train_X, dtype='float64')\n",
    "    train_Y = np.array(train_Y, dtype='float64')\n",
    " \n",
    "    test_X, test_Y = [], []\n",
    "    i = data.shape[0]-n_predictions-n_next-1\n",
    "    a = data[i:(i + n_predictions), :]\n",
    "    test_X.append(a)\n",
    "    tempb = data[(i + n_predictions):(i + n_predictions + n_next), :]\n",
    "    b = []\n",
    "    for j in range(len(tempb)):\n",
    "        for k in range(dim):\n",
    "            b.append(tempb[j, k])\n",
    "    test_Y.append(b)\n",
    "    test_X = np.array(test_X, dtype='float64')\n",
    "    test_Y = np.array(test_Y, dtype='float64')\n",
    " \n",
    "    return train_X, train_Y, test_X, test_Y\n",
    " \n",
    "def NormalizeMult(data, set_range):\n",
    "    '''\n",
    "         Return the normalized data and the maximum and minimum values\n",
    "    '''\n",
    "    normalize = np.arange(2*data.shape[1], dtype='float64')\n",
    "    normalize = normalize.reshape(data.shape[1], 2)\n",
    " \n",
    "    for i in range(0, data.shape[1]):\n",
    "        if set_range == True:\n",
    "            list = data[:, i]\n",
    "            listlow, listhigh = np.percentile(list, [0, 100])\n",
    "        else:\n",
    "            if i == 0:\n",
    "                listlow = -90\n",
    "                listhigh = 90\n",
    "            else:\n",
    "                listlow = -180\n",
    "                listhigh = 180\n",
    " \n",
    "        normalize[i, 0] = listlow\n",
    "        normalize[i, 1] = listhigh\n",
    " \n",
    "        delta = listhigh - listlow\n",
    "        if delta != 0:\n",
    "            for j in range(0, data.shape[0]):\n",
    "                data[j, i] = (data[j, i] - listlow)/delta\n",
    " \n",
    "    return data, normalize\n",
    " \n",
    "def trainModel(train_X, train_Y):\n",
    "    '''\n",
    "         trainX, trainY: the data needed to train the LSTM model\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        120,\n",
    "        input_shape=(train_X.shape[1], train_X.shape[2]),\n",
    "        return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(LSTM(\n",
    "        120,\n",
    "        return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(Dense(\n",
    "        train_Y.shape[1]))\n",
    "    model.add(Activation(\"relu\"))\n",
    " \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['acc'])\n",
    "    model.fit(train_X, train_Y, epochs=100, batch_size=64, verbose=1)\n",
    "    model.summary()\n",
    " \n",
    "    return model\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    train_num = 6\n",
    "    per_num = 1\n",
    "    # set_range = False\n",
    "    set_range = True\n",
    " \n",
    "    # Read in time series file data\n",
    "    data = pd.read_csv(path+file).iloc[:, 0:2].values\n",
    "    print(\"Number of samples: {0}, dimension: {1}\".format(data.shape[0], data.shape[1]))\n",
    "    # print(data)\n",
    " \n",
    "    # Painting sample database\n",
    "    plt.scatter(data[:, 1], data[:, 0], c='b', marker='o', label='traj_A')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    " \n",
    "    #Normalized \n",
    "    data, normalize = NormalizeMult(data, set_range)\n",
    "    # print(normalize)\n",
    " \n",
    "    #Generate training data\n",
    "    train_X, train_Y, test_X, test_Y = create_dataset(data, train_num, per_num)\n",
    "    print(\"x\\n\", train_X.shape)\n",
    "    print(\"y\\n\", train_Y.shape)\n",
    " \n",
    "  \n",
    "    # Training model\n",
    "    model = trainModel(train_X, train_Y)\n",
    "    loss, acc = model.evaluate(train_X, train_Y, verbose=2)\n",
    "    print('Loss : {}, Accuracy: {}'.format(loss, acc * 100))\n",
    " \n",
    "    # Save model\n",
    "    np.save(\"./traj_model_trueNorm.npy\", normalize)\n",
    "    model.save(\"./traj_model_120.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f151dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "def mse(predictions, targets):\n",
    "    return ((predictions - targets) ** 2).mean()\n",
    " \n",
    "def reshape_y_hat(y_hat,dim):\n",
    "    re_y = []\n",
    "    i = 0\n",
    "    while i < len(y_hat):\n",
    "        tmp = []\n",
    "        for j in range(dim):\n",
    "            tmp.append(y_hat[i+j])\n",
    "        i = i + dim\n",
    "        re_y.append(tmp)\n",
    "    re_y = np.array(re_y, dtype='float64')\n",
    "    return re_y\n",
    " \n",
    " #Multidimensional denormalization\n",
    "def FNormalizeMult(data,normalize):\n",
    " \n",
    "    data = np.array(data, dtype='float64')\n",
    "         #Column\n",
    "    for i in range(0, data.shape[1]):\n",
    "        listlow = normalize[i, 0]\n",
    "        listhigh = normalize[i, 1]\n",
    "        delta = listhigh - listlow\n",
    "        print(\"listlow, listhigh, delta\", listlow, listhigh, delta)\n",
    "                 #Row \n",
    "        if delta != 0:\n",
    "            for j in range(0, data.shape[0]):\n",
    "                data[j, i] = data[j, i]*delta + listlow\n",
    " \n",
    "    return data\n",
    " \n",
    " #Using normalization of training data\n",
    "def NormalizeMultUseData(data,normalize):\n",
    " \n",
    "    for i in range(0, data.shape[1]):\n",
    " \n",
    "        listlow = normalize[i, 0]\n",
    "        listhigh = normalize[i, 1]\n",
    "        delta = listhigh - listlow\n",
    " \n",
    "        if delta != 0:\n",
    "            for j in range(0, data.shape[0]):\n",
    "                data[j, i] = (data[j, i] - listlow)/delta\n",
    " \n",
    "    return data\n",
    " \n",
    "from math import sin, asin, cos, radians, fabs, sqrt\n",
    "EARTH_RADIUS = 6371 # The average radius of the earth, 6371km\n",
    " \n",
    " # Calculate the straight-line distance between two latitude and longitude\n",
    "def hav(theta):\n",
    "    s = sin(theta / 2)\n",
    "    return s * s\n",
    "def get_distance_hav(lat0, lng0, lat1, lng1):\n",
    "    # \"Use the haversine formula to calculate the distance between two points on the sphere.\"\n",
    "    # Longitude and latitude converted to radians\n",
    "    lat0 = radians(lat0)\n",
    "    lat1 = radians(lat1)\n",
    "    lng0 = radians(lng0)\n",
    "    lng1 = radians(lng1)\n",
    " \n",
    "    dlng = fabs(lng0 - lng1)\n",
    "    dlat = fabs(lat0 - lat1)\n",
    "    h = hav(dlat) + cos(lat0) * cos(lat1) * hav(dlng)\n",
    "    distance = 2 * EARTH_RADIUS * asin(sqrt(h))\n",
    "    return distance\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    test_num = 6\n",
    "    per_num = 1\n",
    "    data_all = pd.read_csv(path+file).iloc[-2*(test_num+per_num):-1*(test_num+per_num), 0:2].values\n",
    "    data_all.dtype = 'float64'\n",
    " \n",
    "    data = copy.deepcopy(data_all[:-per_num, :])\n",
    "    y = data_all[-per_num:, :]\n",
    "  \n",
    "    # #Normalized \n",
    "    normalize = np.load(\"./traj_model_trueNorm.npy\")\n",
    "    data = NormalizeMultUseData(data, normalize)\n",
    " \n",
    "    model = load_model(\"./traj_model_120.h5\")\n",
    "    test_X = data.reshape(1, data.shape[0], data.shape[1])\n",
    "    y_hat = model.predict(test_X)\n",
    "    location_y = y_hat\n",
    "    print(\"Pred coord:\" ,y_hat)\n",
    "    y_hat = y_hat.reshape(y_hat.shape[1])\n",
    "    y_hat = reshape_y_hat(y_hat, 2)\n",
    " \n",
    "    #Antinormalization\n",
    "    y_hat = FNormalizeMult(y_hat, normalize)\n",
    "    print(\"predict: {0}\\ntrue：{1}\".format(y_hat, y))\n",
    "    print('Mean square error of prediction:', mse(y_hat, y))\n",
    "    print('Predicted straight-line distance: {:.4f} KM'.format(get_distance_hav(y_hat[0, 0], y_hat[0, 1], y[0, 0], y[0, 1])))\n",
    " \n",
    "   \n",
    "    # Draw test sample database\n",
    "    p1 = plt.scatter(data_all[:-per_num, 1], data_all[:-per_num, 0], c='b', marker='o', label='traj_A')\n",
    "    p2 = plt.scatter(y_hat[:, 1], y_hat[:, 0], c='r', marker='o', label='pre')\n",
    "    p3 = plt.scatter(y[:, 1], y[:, 0], c='g', marker='o', label='pre_true')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb0ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_y\n",
    "data = [[1.0000103, 0.9615487]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61621b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f095164",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns = ['latitude', 'longitude'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c95662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(df, lat=\"latitude\", lon=\"longitude\" ,\n",
    "                        color_discrete_sequence=[\"fuchsia\"], zoom=3, height=300)\n",
    "fig.update_geos(fitbounds=\"locations\", scope=\"south america\")\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c2565e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
