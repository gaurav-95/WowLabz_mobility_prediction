{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c5305ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\Gaurav\\Downloads\\WowLabz\\WowLabz_mobility_prediction\\Dataset\"\n",
    "file = r\"\\go_track_trackspoints.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db56c770",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e0f6fe3a18ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import  keras.callbacks\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "#Set to self-growth\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    " \n",
    "def trainModel(train_X, train_Y):\n",
    "    '''\n",
    "         trainX, trainY: the data needed to train the LSTM model\n",
    "    '''\n",
    "    model = Sequential() # Define a stacked sequential model\n",
    "    model.add(LSTM(\n",
    "        240,\n",
    "        input_shape=(train_X.shape[1], train_X.shape[2]),\n",
    "        return_sequences=True)) \n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(LSTM(\n",
    "        240,\n",
    "        return_sequences=True))  \n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(LSTM(\n",
    "        240,\n",
    "                 return_sequences=False)) # Return a single vector with dimension 240\n",
    "    model.add(Dropout(0.3))\n",
    " \n",
    "    model.add(Dense(\n",
    "        train_Y.shape[1]))\n",
    "    model.add(Activation(\"relu\"))\n",
    " \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['acc']) # Configure the model learning process\n",
    " \n",
    "    model.fit(train_X, train_Y, epochs=100, batch_size=64, verbose=1, validation_data=(test_X, test_Y))\n",
    "    model.summary()\n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8139c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_num = 6\n",
    "    per_num = 1\n",
    "    # set_range = False\n",
    "    set_range = True\n",
    "    series_idx = ['latitude', 'longitude']\n",
    "    # Read in time series file data\n",
    "    data = pd.read_csv(path+file).loc[:, series_idx].values\n",
    "    print(\"Number of samples: {0}, dimension: {1}\".format(data.shape[0], data.shape[1]))\n",
    "    # print(data)\n",
    " \n",
    "    # Painting sample database\n",
    "    plt.scatter(data[:, 1], data[:, 0], c='b', marker='o', label='traj_A')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    " \n",
    "    #Normalized \n",
    "    data, normalize = NormalizeMult(data, set_range)\n",
    "    # print(normalize)\n",
    " \n",
    "    #Generate training data\n",
    "    train_X, train_Y, test_X, test_Y = create_dataset(data, train_num, per_num)\n",
    "    print(\"x\\n\", train_X.shape)\n",
    "    print(\"y\\n\", train_Y.shape)\n",
    " \n",
    " \n",
    "    # Training model\n",
    "    model = trainModel(train_X, train_Y)\n",
    "    loss, acc = model.evaluate(train_X, train_Y, verbose=2)\n",
    "    print('Loss : {}, Accuracy: {}'.format(loss, acc * 100))\n",
    " \n",
    "    # Save model\n",
    "    #np.save(\"./traj_model_trueNorm.npy\", normalize)\n",
    "    #model.save(\"./traj_model_240_3layers_altitude.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f20df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "import tensorflow as tf\n",
    "import  pandas as pd\n",
    "import  os\n",
    "import  keras.callbacks\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    " \n",
    " #Set to self-growth\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    " \n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "def mse(predictions, targets):\n",
    "    return ((predictions - targets) ** 2).mean()\n",
    " \n",
    "def reshape_y_hat(y_hat,dim):\n",
    "    re_y = []\n",
    "    i = 0\n",
    "    while i < len(y_hat):\n",
    "        tmp = []\n",
    "        for j in range(dim):\n",
    "            tmp.append(y_hat[i+j])\n",
    "        i = i + dim\n",
    "        re_y.append(tmp)\n",
    "    re_y = np.array(re_y, dtype='float64')\n",
    "    return re_y\n",
    " \n",
    "#Multidimensional denormalization\n",
    "def FNormalizeMult(data,normalize):\n",
    " \n",
    "    data = np.array(data, dtype='float64')\n",
    "         #Column\n",
    "    for i in range(0, data.shape[1]):\n",
    "        listlow = normalize[i, 0]\n",
    "        listhigh = normalize[i, 1]\n",
    "        delta = listhigh - listlow\n",
    "        print(\"listlow, listhigh, delta\", listlow, listhigh, delta)\n",
    "                 #Row \n",
    "        if delta != 0:\n",
    "            for j in range(0, data.shape[0]):\n",
    "                data[j, i] = data[j, i]*delta + listlow\n",
    " \n",
    "    return data\n",
    " \n",
    "#Using normalization of training data\n",
    "def NormalizeMultUseData(data,normalize):\n",
    " \n",
    "    for i in range(0, data.shape[1]):\n",
    " \n",
    "        listlow = normalize[i, 0]\n",
    "        listhigh = normalize[i, 1]\n",
    "        delta = listhigh - listlow\n",
    " \n",
    "        if delta != 0:\n",
    "            for j in range(0, data.shape[0]):\n",
    "                data[j, i] = (data[j, i] - listlow)/delta\n",
    " \n",
    "    return data\n",
    " \n",
    "from math import sin, asin, cos, radians, fabs, sqrt\n",
    " \n",
    "EARTH_RADIUS = 6371 # The average radius of the earth, 6371km\n",
    " \n",
    "# Calculate the straight-line distance between two latitude and longitude\n",
    "def hav(theta):\n",
    "    s = sin(theta / 2)\n",
    "    return s * s\n",
    "def get_distance_hav(lat0, lng0, lat1, lng1):\n",
    "         # \"Use the haversine formula to calculate the distance between two points on the sphere.\"\n",
    "         # Longitude and latitude converted to radians\n",
    "    lat0 = radians(lat0)\n",
    "    lat1 = radians(lat1)\n",
    "    lng0 = radians(lng0)\n",
    "    lng1 = radians(lng1)\n",
    " \n",
    "    dlng = fabs(lng0 - lng1)\n",
    "    dlat = fabs(lat0 - lat1)\n",
    "    h = hav(dlat) + cos(lat0) * cos(lat1) * hav(dlng)\n",
    "    distance = 2 * EARTH_RADIUS * asin(sqrt(h))\n",
    "    return distance\n",
    " \n",
    "if __name__ == '__main__':\n",
    "    test_num = 6\n",
    "    per_num = 1\n",
    "    series_idx = ['latitude', 'longitude']\n",
    "    test_move_loc = 0\n",
    "    data_all = pd.read_csv(path+file)\n",
    "    data_all = data_all.loc[np.arange(len(data_all)-test_num-per_num-test_move_loc, len(data_all)-test_move_loc).tolist(), series_idx].values\n",
    "    # print(data_all)\n",
    "    data_all.dtype = 'float64'\n",
    " \n",
    "    data = copy.deepcopy(data_all[:-per_num, :])\n",
    "    y = data_all[-per_num:, :]\n",
    "  \n",
    "         # #Normalized \n",
    "    normalize = np.load(\"./traj_model_trueNorm.npy\")\n",
    "    data = NormalizeMultUseData(data, normalize)\n",
    " \n",
    "    model = load_model(\"./traj_model_120.h5\")\n",
    "    test_X = data.reshape(1, data.shape[0], data.shape[1])\n",
    "    y_hat = model.predict(test_X)\n",
    "    y_hat = y_hat.reshape(y_hat.shape[1])\n",
    "    y_hat = reshape_y_hat(y_hat, y.shape[1])\n",
    " \n",
    "    #Antinormalization\n",
    "    y_hat = FNormalizeMult(y_hat, normalize)\n",
    "    print(\"predict: {0}\\ntrueï¼š{1}\".format(y_hat, y))\n",
    "    # print('Prediction mean square error:', mse(y_hat, y))\n",
    "    print('Prediction mean square error:', mse(y_hat[-1, :], y[-1, :]))\n",
    "    # print('Predicted straight-line distance: {:.4f} KM'.format(get_distance_hav(y_hat[0, 0], y_hat[0, 1], y[0, 0], y[0, 1])))\n",
    "    print('Predicted straight-line distance: {:.4f} KM'.format(get_distance_hav(y_hat[-1, 0], y_hat[-1, 1], y[-1, 0], y[-1, 1] )))\n",
    "    print('Predicted height difference: {:.4f} M'.format((y_hat[-1, 2]-y[-1, 2]) * 0.3047999995367)) # 1 feet = 0.3047999995367 m\n",
    " \n",
    "   \n",
    "    # Draw test sample database\n",
    "    p1 = plt.scatter(y_hat[:, 1], y_hat[:, 0], c='r', marker='o', label='pre')\n",
    "    p2 = plt.scatter(y[:, 1], y[:, 0], c='g', marker='o', label='pre_true')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
